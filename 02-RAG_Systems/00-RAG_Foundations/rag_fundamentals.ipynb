{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "954295cd",
   "metadata": {},
   "source": [
    "### RAG - FUNDAMENTALS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6418c0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ‚è≥ The Case of the Outdated Manual\n",
    "\n",
    "Imagine an employee named **Alex**. Alex works for a major technology company that has thousands of internal documents‚Äîmanuals, policies, and quarterly reports‚Äîspread across ancient shared drives.\n",
    "\n",
    "Alex is building a critical report for a client and needs to know the warranty period for a specific product line, the **Alpha 7** model.\n",
    "\n",
    "* **Alex turns to the company's AI Chatbot.** This chatbot was trained years ago on a massive, general dataset.\n",
    "* **Query:** \"What is the warranty period for the Alpha 7?\"\n",
    "* **The AI's Reply (without RAG):** \"The standard warranty is 18 months, according to the general hardware terms I was trained on.\"\n",
    "\n",
    "Alex feels confident... until the real world hits. The Alpha 7 was part of a special program launched *last month* where the warranty was **extended to 36 months**. The AI was confident, but it was confidentially **wrong**. It was stuck in the past.\n",
    "\n",
    "***\n",
    "\n",
    "**(Slide 2: Transition Slide - The Problem and the Solution)**\n",
    "\n",
    "This challenge‚Äîthe confident, well-spoken, yet factually outdated AI‚Äîis the problem we face with Large Language Models (LLMs) alone.\n",
    "\n",
    "**The Solution?** We don't need to retrain the entire AI every time a new document is published. We need to give it a modern **library card**.\n",
    "\n",
    "We need **Retrieval Augmented Generation (RAG).**\n",
    "\n",
    "### RAG: The Real-Time Librarian\n",
    "\n",
    "RAG gives the AI a mechanism to instantly look up the single most current and relevant piece of information‚Äîlike that updated 36-month warranty policy‚Äîfrom our private, live knowledge base **before** it generates the answer.\n",
    "\n",
    "***\n",
    "\n",
    "**(Slide 3: Real-World Usefulness of RAG)**\n",
    "\n",
    "### üåé Beyond the Manual: Where RAG is a Must-Have\n",
    "\n",
    "RAG is how companies are making AI safe, accurate, and truly useful today.\n",
    "\n",
    "| Domain | The Challenge Solved by RAG | Example of Grounded Answer |\n",
    "| :---: | :---: | :---: |\n",
    "| **Customer Support** | Getting real-time answers about new products, returns, and FAQs. | *Instead of a generic answer,* a chatbot answers with the **exact steps** from the **latest** refund policy document. |\n",
    "| **Finance & Legal** | Keeping analysts updated on changing regulations and reports. | *Instead of summarizing a general concept,* a system summarizes the **new compliance ruling** from the **Q4 2024** regulatory filing. |\n",
    "| **Healthcare** | Grounding diagnoses in the very latest medical research. | *Instead of a general description,* an AI provides a diagnosis supported by **citations** from a medical journal published **yesterday**. |\n",
    "\n",
    "**In short: RAG ensures our LLMs stop relying on outdated memory and start grounding their intelligence in verifiable, up-to-the-minute facts.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45ae33",
   "metadata": {},
   "source": [
    "### üß† LLMs: The RAG Advantage\n",
    "\n",
    "| Without RAG | With RAG |\n",
    "| :---: | :---: |\n",
    "| **THE ISOLATED STUDENT** | **THE INFORMED EXPERT** |\n",
    "| ‚ùå **Relying Solely on Memory** | ‚úÖ **Accessing the \"Open Textbook\"** |\n",
    "| Knowledge is **Outdated** or **Fuzzy** | Knowledge is **Current** and **Fact-Checked** |\n",
    "| **Result:** Potential for **Hallucination** | **Result:** Enhanced **Accuracy** and **Trust** |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c63d5c4",
   "metadata": {},
   "source": [
    "#### RAG ARCHITECTURE\n",
    "<div style=\"background-color: white; text-align: center;\">\n",
    "<img src=\"../../assests/02-RAG_Systems/RAG_ARCH.png\" alt=\"Plot 1\" width=\"700\" style=\"background-color: white;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0338ac",
   "metadata": {},
   "source": [
    "# üìö Understanding RAG: The Architecture\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that gives Large Language Models (LLMs) access to private or up-to-date data that wasn't in their training set.\n",
    "\n",
    "Think of it like an **Open Book Exam**:\n",
    "* **The LLM:** The student (smart, but doesn't know specific internal details).\n",
    "* **The Vector DB:** The textbook.\n",
    "* **RAG:** The process of looking up the relevant chapter before answering the question.\n",
    "\n",
    "### The Workflow at a Glance\n",
    "Referencing the architecture diagram above, we will break the process into three distinct stages:\n",
    "1.  **Ingestion:** Preparing the \"textbook\" (Yellow/Green top path).\n",
    "2.  **Retrieval:** Finding the right \"page\" (Blue middle box).\n",
    "3.  **Generation:** Writing the answer (Blue/Grey bottom box)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df75560",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Phase 1: Ingestion (Preprocessing)\n",
    "*Refer to the top branch of the diagram.*\n",
    "\n",
    "Before we can search our data, we must prepare it. LLMs have a \"Context Window\" limit, meaning we cannot feed them an entire book at once. We must break it down.\n",
    "\n",
    "### Key Steps:\n",
    "1.  **Documents:** We load raw text (PDFs, TXT, HTML).\n",
    "2.  **Chunking Algorithm (üî¥ Red Box):** We split the text into smaller, manageable pieces (e.g., 500 words).\n",
    "    * *Why?* If chunks are too small, we lose context. If too large, we retrieve irrelevant noise.\n",
    "3.  **Embedding Model:** This converts text into **Vectors** (lists of numbers).\n",
    "    * *Concept:* $$\\text{\"Concrete\"} \\rightarrow [0.1, 0.5, 0.9]$$\n",
    "4.  **Vector DB:** We store these numbers for fast searching.\n",
    "\n",
    "> **Note:** The quality of your RAG system is heavily dependent on your **Chunking Strategy**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dbd8fb",
   "metadata": {},
   "source": [
    "## üîç Phase 2: Retrieval\n",
    "*Refer to the middle \"Retrieval\" box in the diagram.*\n",
    "\n",
    "When a user asks a question, we don't send it straight to the LLM yet. We first need to find relevant information.\n",
    "\n",
    "### The \"Semantic Search\" Process:\n",
    "1.  **User Question:** The user inputs a query.\n",
    "2.  **Input Embedding:** We use the **same** embedding model from Phase 1 to turn the question into a vector.\n",
    "3.  **Retrieval Algorithm (üî¥ Red Box):** We compare the *Question Vector* against our database of *Document Vectors*.\n",
    "    * We use math (typically **Cosine Similarity**) to find the closest matches.\n",
    "    * $$\\text{similarity} = \\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n",
    "4.  **Top-N Relevant Content:** The system returns the top 3-5 chunks that are mathematically closest in meaning to the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b83f7f",
   "metadata": {},
   "source": [
    "## üß† Phase 3: Augmentation & Generation\n",
    "*Refer to the bottom \"Augmentation\" and \"Generation\" boxes.*\n",
    "\n",
    "This is where the magic happens. We combine the user's intent with the retrieved data.\n",
    "\n",
    "### 1. Augmentation (Prompt Construction)\n",
    "We don't just paste the chunks. We wrap them in a **Prompt Template** to guide the LLM.\n",
    "A typical structure looks like this:\n",
    "\n",
    "```text\n",
    "You are a helpful assistant. Answer the user question based ONLY on the context provided below.\n",
    "\n",
    "Context:\n",
    "{Top_N_Relevant_Content}\n",
    "\n",
    "Question:\n",
    "{User_Question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d6e51a",
   "metadata": {},
   "source": [
    "## üíª Pseudo-Code for Simple RAG system\n",
    "```python\n",
    "\n",
    "# 1. Ingestion\n",
    "documents = load_pdfs(\"data/\")\n",
    "chunks = split_text(documents, chunk_size=500)\n",
    "vector_db = store_embeddings(chunks)\n",
    "\n",
    "# 2. Retrieval\n",
    "user_question = \"What is the warranty period?\"\n",
    "question_vector = embed(user_question)\n",
    "relevant_chunks = vector_db.similarity_search(question_vector, k=3)\n",
    "\n",
    "# 3. Generation\n",
    "prompt = f\"\"\"\n",
    "Answer based on this context: {relevant_chunks}\n",
    "Question: {user_question}\n",
    "\"\"\"\n",
    "response = llm.generate(prompt)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
