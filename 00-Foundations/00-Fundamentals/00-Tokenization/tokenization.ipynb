{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6be0ac1",
   "metadata": {},
   "source": [
    "### TOKENIZATION - INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd923e5",
   "metadata": {},
   "source": [
    "#### How Machine Read Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b62d30",
   "metadata": {},
   "source": [
    "**Analogy:** Imagine teaching a child to read. You don't start with whole sentences. You start with letters (A, B, C) or sounds (phonics).\n",
    "\n",
    "1. **Tokenization** is similar for AI. It breaks text into smaller chunks called tokens.\n",
    "\n",
    "2.  These tokens are then converted into numbers (IDs) because models like GPT-4 or Llama 3 only understand math, not words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63445e04",
   "metadata": {},
   "source": [
    "#### The Three Types of Tokenization\n",
    "1. **Word Tokenization:** Splitting by spaces.\n",
    "\n",
    "*Problem:* \"Apple\" and \"Apples\" are treated as totally unrelated words. Huge vocabulary required.\n",
    "\n",
    "2. **Character Tokenization:** Splitting by letter.\n",
    "\n",
    "*Problem:* Sequences become too long (100 words = 500+ characters). \"Apple\" loses its meaning when split into 'a', 'p', 'p', 'l', 'e'.\n",
    "\n",
    "3. **Subword Tokenization (The Standard):** A mix of both.\n",
    "\n",
    "*Solution:* Common words (\"apple\") stay as one token. Rare words (\"fingerprinted\") split into parts (\"finger\", \"print\", \"ed\").\n",
    "\n",
    "Benefit: Efficient and handles unknown words well. This is what GPT and BERT use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef8d00",
   "metadata": {},
   "source": [
    "#### Example 1: Space vs. Character Split (The \"Old Way\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0db62789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['This', 'is', 'a', 'sample', 'tokenization', 'tutorial']\n",
      "Character Tokens: ['T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', ' ']...\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a sample tokenization tutorial\"\n",
    "\n",
    "# 1. Word Tokenization (Naive)\n",
    "word_tokens = text.split()\n",
    "print(f\"Word Tokens: {word_tokens}\")\n",
    "# Critique: Punctuation is stuck to words ('Unbelievably,'), making it messy.\n",
    "\n",
    "# 2. Character Tokenization\n",
    "char_tokens = list(text)\n",
    "print(f\"Character Tokens: {char_tokens[:10]}...\") # Printing just first 10\n",
    "# Critique: The list is huge and individual letters represent no meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170d9363",
   "metadata": {},
   "source": [
    "#### Example 2: Tokenization with OpenAI's tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0fb122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [2028, 374, 264, 6205, 4037, 2065, 22237]\n",
      "--------------------------------\n",
      "Mapping IDs back to Text chunks:\n",
      "2028 -> 'This'\n",
      "374 -> ' is'\n",
      "264 -> ' a'\n",
      "6205 -> ' sample'\n",
      "4037 -> ' token'\n",
      "2065 -> 'ization'\n",
      "22237 -> ' tutorial'\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Load the tokenizer used by GPT-4 (cl100k_base)\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "tokens = encoder.encode(text)\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "print(\"Mapping IDs back to Text chunks:\")\n",
    "for t in tokens:\n",
    "    print(f\"{t} -> '{encoder.decode([t])}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7734a85",
   "metadata": {},
   "source": [
    "#### Example 3: Tokenization with Hugging Face (BERT/Llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "853d2413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/murugesan.vadivel/DEV/works/VGLUG_Talk_Nov_2025/My_GenAI_Garage/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['token', '##ization', 'is', 'fun', '!']\n",
      "Token IDs: [19204, 3989, 2003, 4569, 999]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a popular tokenizer (BERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Tokenization is fun!\"\n",
    "\n",
    "# 1. Tokenize (Text to Strings)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# 2. Convert to IDs (Strings to Numbers)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"Token IDs: {ids}\")\n",
    "\n",
    "# Observation:\n",
    "# Notice how BERT handles capital letters (it lowers them) and distinct tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1403db23",
   "metadata": {},
   "source": [
    "#### Visualizing the \"Subword\" Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d795f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Tokenization is unputdownable\n",
      "\n",
      "Breakdown:\n",
      "ID: 3404     | Token: 'Token'\n",
      "ID: 2065     | Token: 'ization'\n",
      "ID: 374      | Token: ' is'\n",
      "ID: 653      | Token: ' un'\n",
      "ID: 631      | Token: 'put'\n",
      "ID: 2996     | Token: 'down'\n",
      "ID: 481      | Token: 'able'\n"
     ]
    }
   ],
   "source": [
    "# Let's use GPT-4's tokenizer again\n",
    "text_complex = \"Tokenization is unputdownable\" \n",
    "# 'unputdownable' is a rare word (meaning a book you can't put down)\n",
    "\n",
    "tokens = encoder.encode(text_complex)\n",
    "\n",
    "print(f\"Original Text: {text_complex}\")\n",
    "print(\"\\nBreakdown:\")\n",
    "\n",
    "for t in tokens:\n",
    "    part = encoder.decode([t])\n",
    "    print(f\"ID: {t:<8} | Token: '{part}'\")\n",
    "    \n",
    "# Expected Result Explanation:\n",
    "# Common words like \"Token\" might stay whole.\n",
    "# Complex words like \"unputdownable\" will likely get split into:\n",
    "# 'un', 'put', 'down', 'able' (or similar variations).\n",
    "# This proves the AI understands the *structure* of words it hasn't seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f0042b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
