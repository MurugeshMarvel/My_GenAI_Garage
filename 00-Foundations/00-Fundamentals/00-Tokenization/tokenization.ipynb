{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6be0ac1",
   "metadata": {},
   "source": [
    "### TOKENIZATION - INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd923e5",
   "metadata": {},
   "source": [
    "#### How Machine Read Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b62d30",
   "metadata": {},
   "source": [
    "**Analogy:** Imagine teaching a child to read. You don't start with whole sentences. You start with letters (A, B, C) or sounds (phonics).\n",
    "\n",
    "1. **Tokenization** is similar for AI. It breaks text into smaller chunks called tokens.\n",
    "\n",
    "2.  These tokens are then converted into numbers (IDs) because models like GPT-4 or Llama 3 only understand math, not words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63445e04",
   "metadata": {},
   "source": [
    "#### The Three Types of Tokenization\n",
    "1. **Word Tokenization:** Splitting by spaces.\n",
    "\n",
    "*Problem:* \"Apple\" and \"Apples\" are treated as totally unrelated words. Huge vocabulary required.\n",
    "\n",
    "2. **Character Tokenization:** Splitting by letter.\n",
    "\n",
    "*Problem:* Sequences become too long (100 words = 500+ characters). \"Apple\" loses its meaning when split into 'a', 'p', 'p', 'l', 'e'.\n",
    "\n",
    "3. **Subword Tokenization (The Standard):** A mix of both.\n",
    "\n",
    "*Solution:* Common words (\"apple\") stay as one token. Rare words (\"fingerprinted\") split into parts (\"finger\", \"print\", \"ed\").\n",
    "\n",
    "Benefit: Efficient and handles unknown words well. This is what GPT and BERT use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef8d00",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
